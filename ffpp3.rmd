

```{r echo=FALSE}
# This removes all items in environment. 
rm(list=ls())
library(vembedr)
```

# Introduction. {.unnumbered}

Forecast methods are essential in business and finance for strategic decision-making, efficient resource allocation, and financial planning. Accurate forecasts enable organizations to identify and manage risks, attract investor confidence, stay competitive in the market, optimize supply chains, and evaluate performance against benchmarks. By anticipating changes in the business environment, businesses can adapt proactively and comply with regulatory requirements. Overall, forecasting provides a structured approach to planning, allowing businesses to navigate uncertainties and achieve long-term success.

# The forecast problem.

The problem is to forecast a time series. In particular, the time series is the *Beer, Wine, and Distilled Alcoholic Beverages Sales* as in the original Matt Dancho's example. The data is taken from FRED (Federal Reserve Economic Data). The data belongs to the non-durable goods category, it includes U.S. merchant wholesalers, except manufacturers' sales branches and offices sales. The monthly time series goes from 2010-01-01 to 2022-10-31. And the goal is to use 2022 data (10 months) as a test data to conduct the forecast.

For the full database details see: 
https://fred.stlouisfed.org/series/S4248SM144NCEN

Before start, see: What is forecasting?

```{r}
embed_url("https://youtu.be/zvd9oboayEc?si=DfK9JkSqapE8JHiw")
```

## The data.

Let's load the R packages.

```{r}
# Load libraries
library(fpp3)
library(h2o)        # ML Library.
library(timetk)     # Toolkit for working with time series in R.
library(tidyquant)  # Loads tidyverse, financial pkgs, used to get data.
library(dplyr)      # Database manipulation.
library(ggplot2)    # Plots.
library(tibble)     # Tables.
library(kableExtra) # Tables.
library(knitr)      
library(bit64)      # Useful in the machine learning workflow.
library(sweep)      # Broom-style tidiers for the forecast package.
library(forecast)   # Forecasting models and predictions package.
library(seasonal)
library(tictoc)
```

We can conveniently download the data directly from the FRED API in one line of code.

```{r get-data}
# Beer, Wine, Distilled Alcoholic Beverages, in Millions USD.
beer <- tq_get("S4248SM144NCEN", get = "economic.data", 
               from = "2010-01-01", to = "2022-10-31")
```

Let's have a look of the data set. By default it says *price*, but these are sales figures in monetary terms. According to the main FRED reference, these are in millions of dollars, not seasonally adjusted.

```{r see-the-data}
head(beer)
```

We can change the name of the price column.

```{r ren}
beer <- beer %>%
  rename(sales = price)

tail(beer)
```
Better now. 

Visualization is particularly important for time series analysis and forecasting. It’s a good idea to identify spots where we will split the data into training and test. This kind of split is consistent with most machine learning algorithms. The *training* dataset is the sample of data used to fit and train the model by learning from the data. The *test* dataset is the sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. The test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained. The test set is generally what is used to evaluate competing models.

It is also important to see the time series because normally the models will perform better if we can identify basic characteristics such as trend and seasonality. This data set clearly has a trend and a seasonality as people drink more alcohol in December. 

```{r}
#| label: fig-bwadabs
#| fig-cap: "Beer, Wine, and Distilled Alcoholic Beverages Sales."
beer %>%
  ggplot(aes(date, sales)) +
  # Train Region:
  annotate("text", x = ymd("2013-01-01"), y = 14000, 
           color = "black", label = "Train region") +
  geom_rect(xmin = as.numeric(ymd("2022-01-01")), 
            xmax = as.numeric(ymd("2022-09-30")), ymin = 0, ymax = 20000, 
            alpha = 0.02, fill = "pink") +
  annotate("text", x = ymd("2022-06-01"), y = 9000,
           color = "black", label = "Test\nregion") +
  # Data.
  geom_line(col = "black") +
  geom_point(col = "black", alpha = 0.5, size = 2) +
  # Aesthetics.
  theme_tq() +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  labs(subtitle = 
  "Train (2010 - 2021), and test set (Jan 2022 to Oct 2022)",
  x = "Date", y = "Sales",
       caption = "The models do not know the test region, this is for us 
       to see how well the models do the 10-month ahead forecast.") +
  scale_y_continuous(labels = scales::dollar)
```

Then, the problem is to forecast the 10 months of the test region. This is, from January to October 2022. 

Here is a zoom version of the plot above.

```{r}
#| label: fig-zbwadabs
#| fig-cap: "Zoom: Beer, Wine, and Distilled Alcoholic Beverages Sales."
beer %>%
  filter(date > as.Date("2020-01-01")) %>%
  ggplot(aes(date, sales)) +
  annotate("text", x = ymd("2020-08-01"), y = 17000, 
           color = "black", label = "Train region") +
  geom_rect(xmin = as.numeric(ymd("2022-01-01")), 
            xmax = as.numeric(ymd("2022-09-30")), ymin = 0, ymax = 20000, 
            alpha = 0.02, fill = "pink") +
  annotate("text", x = ymd("2022-05-01"), y = 14000,
           color = "black", label = "Test region") +
  geom_line(col = "black") +
  geom_point(col = "black", alpha = 0.5, size = 5) +
  theme_tq() +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  labs(subtitle = 
  "Train (2010 - 2021) and test set (Jan 2022 to Oct 2022)",
  x = "Date", y = "Sales",
       caption = "The models do not know the test region, this is for us 
       to see how well the models do the 10-month ahead forecast.") +
  scale_y_continuous(labels = scales::dollar)
```

## Time series properties.

The forecasting techniques are expected to exploit the time-series components like trend and seasonal component. Here we use the @hyndman2021forecasting <tt>`fpp3`</tt> package to learn about the time series properties before conducting the forecast techniques. In order to use the <tt>`fpp3`</tt> package, we have to transform <tt>`beer`</tt> from a <tt>`tibble`</tt> to a <tt>`tsibble`</tt> object.

```{r from tibble to tsibble}
beer_tbls <- beer
beer_tbls$date <- yearmonth(beer_tbls$date)
beer_tbls <- as_tsibble(beer_tbls)
```

According to @hyndman2021forecasting, the X-11 method was originated in the US Census Bureau and further developed by Statistics Canada. The decomposition process tends to be highly robust to outliers and level shifts in the time series. The details of the X-11 method are described in @dagum2016seasonal.


```{r}
#| label: fig-amdobsux
#| fig-cap: "A multiplicative decomposition of beer sales using X-11."
beer_tbls %>%
  model(x11 = X_13ARIMA_SEATS(sales ~ x11())) %>%
  components() %>%
  autoplot() +
  labs(y = "Sales", x = "Date") +
  scale_y_continuous(labels = scales::dollar)
```
The trend shows a clear change in the first half of 2020. Let's take a look of it.

```{r}
beer_tbls %>%
  model(x11 = X_13ARIMA_SEATS(sales ~ x11())) %>%
  components() %>%
  select(date, sales, trend) %>%
  filter_index("2020-03" ~ "2020-10") %>%
  head(8)
```
The consumption trend significantly increased from June 2020 to July 2020. It is big enough to create a discontinuity in the trend plot below.

```{r}
#| label: fig-td
#| fig-cap: "Trend discontinuity."
beer_tbls %>%
  model(x11 = X_13ARIMA_SEATS(sales ~ x11())) %>%
  components() %>%
  select(date, trend) %>%
  ggplot(aes(yearmonth(date), trend)) + 
  geom_point(alpha = 0.3, size = 4) +
  geom_point(aes(x = yearmonth("2020 jun."), y = 13950.99), 
             alpha = 0.3, size = 4, colour = "red") +
  geom_point(aes(x = yearmonth("2020 jul."), y = 15356.13), 
             alpha = 0.3, size = 4, colour = "red") +
  labs(y = "Trend", x = "Date") +
  scale_y_continuous(labels = scales::dollar)
```

There are also three negative spikes in the irregular component.

```{r}
beer_tbls %>%
  model(x11 = X_13ARIMA_SEATS(sales ~ x11())) %>%
  components() %>%
  select(date, sales, irregular) %>%
  filter(irregular < 0.95) %>%
  head(3)
```

```{r}
#| label: fig-tnsitic
#| fig-cap: "Three negative spikes in the irregular component."
beer_tbls %>%
  model(x11 = X_13ARIMA_SEATS(sales ~ x11())) %>%
  components() %>%
  select(date, irregular) %>%
  ggplot(aes(yearmonth(date), irregular)) + 
  geom_point(alpha = 0.3, size = 4) +
  geom_point(aes(x = yearmonth("2020 apr."), y = 0.8894670), 
             alpha = 0.3, size = 4, colour = "red") +
  geom_point(aes(x = yearmonth("2020 dec."), y = 0.8688517), 
             alpha = 0.3, size = 4, colour = "red") +
  geom_point(aes(x = yearmonth("2021 dec."), y = 0.9099203), 
             alpha = 0.3, size = 4, colour = "red") +
  labs(y = "Irregular", x = "Date") +
  scale_y_continuous(labels = scales::dollar)
```

Apparently, these trend and irregular events are consistent independently of the decomposition technique. We implement the SEATS decomposition below. SEATS stands for *Seasonal Extraction in ARIMA Time Series*. According to @hyndman2021forecasting, this procedure was developed at the Bank of Spain, and is now widely used by government agencies around the world. See @dagum2016seasonal for further details.

```{r}
#| label: fig-adobsous
#| fig-cap: "A decomposition of beer sales obtained using SEATS."
beer_tbls %>%
  model(seats = X_13ARIMA_SEATS(sales ~ seats())) %>%
  components() %>%
  autoplot() +
  labs(y = "Sales", x = "Date") +
  scale_y_continuous(labels = scales::dollar)
```

A seasonal plot is similar to a time plot except that the data are plotted against the individual *seasons*, in this case months, in which the data were observed. 

```{r}
#| label: fig-spbs
#| fig-cap: "Seasonal plot: Beer sales."
beer_tbls %>%
  gg_season(sales, labels = "both") +
  labs(y = "Sales", x = "Date") +
  scale_y_continuous(labels = scales::dollar)
```
And this is the zoom version of the plot above.

```{r}
#| label: fig-zspbs
#| fig-cap: "Zoom. Seasonal plot: Beer sales."
beer_tbls %>%
  filter_index("2019-01" ~ .) %>%
  gg_season(sales, labels = "both", size = 2) +
  geom_point() +
  labs(y = "Sales", x = "Date") +
  scale_y_continuous(labels = scales::dollar)
```

An alternative plot that emphasises the seasonal patterns is where the data for each month are collected together in separate mini time plots.

```{r}
#| label: fig-sspombs
#| fig-cap: "Seasonal subseries plot of monthly beer sales."
beer_tbls %>%
  gg_subseries(sales) +
  labs(y = "Sales", x = "Date") +
  scale_y_continuous(labels = scales::dollar)
```
December is the month of the year with the highest average sales, followed by June. January is the month of the year with the lowest average sales, followed by February.

Following @hyndman2021forecasting, just as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between lagged values of a time series. When data have a trend, the autocorrelations for small lags tend to be large and positive because observations nearby in time are also nearby in value. So the ACF of a trended time series tends to have positive values that slowly decrease as the lags increase. When data are seasonal, the autocorrelations will be larger for the seasonal lags (at multiples of the seasonal period, in this case 12) than for other lags.

```{r}
#| label: fig-aombs
#| fig-cap: "ACF of monthly beer sales."
beer_tbls %>%
  ACF(sales, lag_max = 60) %>%
  autoplot()
```
Beer sales is both trended and seasonal. The slow decrease in the ACF as the lags increase is due to the trend, while the *scalloped* shape is due to the seasonality.

# Forecasts with <tt>`fpp3`</tt>.

Here we implement some selected forecast techniques using the @hyndman2021forecasting <tt>`fpp3`</tt> package. First define the training and test set (2022).

```{r}
beer_train <- beer_tbls %>%
  select(date, sales) %>%
  filter_index(. ~ "2021-12")

beer_2022 <- beer_tbls %>%
  select(date, sales) %>%
  filter_index("2022-01" ~ .)
```

## Four simple techniques.

Let's estimate four simple forecast techniques. *Mean*, where the forecasts of all future values are equal to the average of the historical data. *Naïve*, we set all forecasts to be the value of the last observation. *Seasonal naïve*, we set each forecast to be equal to the last observed value from the same season. And *drift*, to allow the forecasts to increase or decrease over time.

Estimate the four models.

```{r}
beer_fit <- beer_train %>%
  model("fpp3: mean" = MEAN(sales), "fpp3: naïve" = NAIVE(sales),
    "fpp3: seasonal naïve" = SNAIVE(sales), 
    "fpp3: drift" = RW(sales ~ drift()))
glance(beer_fit)
```

The 10-month forecasts.

```{r}
beer_fc <- beer_fit %>%
  fabletools::forecast(h = "10 months")
```

Let's compute the MAPE of all forecasts.

```{r}
mape_table <- fabletools::accuracy(beer_fc, beer_2022) %>%
  select(.model, MAPE) %>%
  arrange(desc(-MAPE))
mape_table
```

```{r}
sn_mape <- fabletools::accuracy(beer_fc, beer_2022) %>%
  filter(.model == "fpp3: seasonal naïve") %>%
  select(MAPE) %>%
  unlist()
sn_mape
```

Let's plot the forecast results.

```{r}
#| label: fig-ffsf
#| fig-cap: "fpp3: four simple forecasts."
beer_fc %>%
  autoplot(beer_tbls, level = NULL) +
  geom_vline(xintercept = as.Date("2022-01-01"), lty = 2) +
  labs(y = "Sales", x = "Date") +
  guides(colour = guide_legend(title = "Forecast:")) +
  theme(legend.position = "bottom") +
  scale_y_continuous(labels = scales::dollar)
```
The plot above is not very clear. Here is a zoom version.

```{r}
#| label: fig-zffsf
#| fig-cap: "Zoom. fpp3: four simple forecasts."
beer_zoom <- beer_tbls %>%
  select(date, sales) %>%
  filter_index("2019-12" ~ .)

beer_fc %>%
  autoplot(beer_zoom, level = NULL, lwd = 2)  +
  geom_vline(xintercept = as.Date("2022-01-01"), lty = 2) +
  labs(y = "Sales", x = "Date") +
  guides(colour = guide_legend(title = "Forecast:")) +
  theme(legend.position = "bottom")
```
This is the seasonal naïve forecasts.

```{r}
#| label: fig-zfsnf
#| fig-cap: "Zoom. fpp3: seasonal naïve forecasts."
beer_zoom <- beer_tbls %>%
  select(date, sales) %>%
  filter_index("2019-12" ~ .)

beer_sn_fc <- beer_fc %>%
  filter(.model == "fpp3: seasonal naïve") 

  ggplot(beer_zoom, aes(yearmonth(date), sales), lwd = 2, alpha = 0.4) +
  geom_line() +
  geom_point(size = 5, color = "black", alpha = 0.5, 
             shape = 21, fill = "black") +
  geom_point(aes(y = .mean), size = 5, 
             color = "red", alpha = 0.5, shape = 21, 
             fill = "red", data = beer_sn_fc) +
  geom_line(aes(y = .mean), color = "red", size = 0.5, data = beer_sn_fc) +
    geom_vline(xintercept = as.numeric(as.Date("2021-12-01")), 
               linetype = 2) +
  labs(y = "Sales", x = "Date",
       caption = c(paste("MAPE=",(round(sn_mape, 5))))) +
  theme(legend.position = "bottom") +
    scale_y_continuous(labels = scales::dollar)
```
The simple techniques are not necessarily bad techniques. 

## Exponential smoothing.

According to @hyndman2021forecasting, forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation the higher the associated weight. This framework generates reliable forecasts quickly and for a wide range of time series, which is a great advantage and of major importance to applications in industry.

In this subsection, we let the <tt>`ETS()`</tt> function select the model by minimising the AICc.

```{r}
beer_ets <- beer_train %>%
  model(ETS(sales))
report(beer_ets)
```

The <tt>`ETS(M,A,M)`</tt> corresponds to a Holt-Winters multiplicative method with multiplicative errors for when seasonal variations are changing proportional to the level of the series.

```{r}
#| label: fig-fec
#| fig-cap: "fpp3: ETS components."
components(beer_ets) %>%
  autoplot() +
  labs(x = "Date", y = "Sales") +
  scale_y_continuous(labels = scales::dollar)
```

The ETS(M,A,M) 10-month forecast.

```{r}
beer_ets_fc <- beer_ets %>%
  fabletools::forecast(h = 10)
beer_ets_fc
```

The ETS(M,A,M) MAPE.

```{r}
ets_mape <- fabletools::accuracy(beer_ets_fc, beer_2022) %>%
  select(MAPE) %>%
  unlist()
ets_mape
```
Let's see the <tt>`ETS(M,A,M)`</tt> forecast.

```{r}
#| label: fig-fef
#| fig-cap: "fpp3: ETS(M,A,M) forecast."
beer_ets %>%
  fabletools::forecast(h = 10) %>%
  autoplot(beer_tbls) +
  geom_vline(xintercept = as.Date("2022-01-01"), lty = 2) +
  labs(x = "Date", y = "Sales") +
  theme(legend.position = "bottom") +
  scale_y_continuous(labels = scales::dollar)
```
This is not very clear, here is the zoom version.

```{r}
#| label: fig-zfef
#| fig-cap: "Zoom. fpp3: ETS(M,A,M) forecast."
  ggplot(beer_zoom, aes(yearmonth(date), sales), lwd = 2, alpha = 0.4) +
  geom_line() +
  geom_point(size = 5, color = "black", alpha = 0.5, 
             shape = 21, fill = "black") +
  geom_point(aes(y = .mean), size = 5, 
             color = "red", alpha = 0.5, shape = 21, 
             fill = "red", data = beer_ets_fc) +
  geom_line(aes(y = .mean), color = "red", size = 0.5, data = beer_ets_fc) +
  geom_vline(xintercept = as.numeric(as.Date("2021-12-01")), 
               linetype = 2) +
  labs(x = "Date", y = "Sales",
       caption = c(paste("MAPE=",(round(ets_mape, 5))))) +
  theme(legend.position = "bottom") +
  scale_y_continuous(labels = scales::dollar)
```
Update the MAPE table.

```{r}
mape_updated <- mape_table %>%
  add_row(.model = "fpp3: ETS(M,A,M)", MAPE = ets_mape) %>%
  arrange(desc(-MAPE))
mape_updated
```

## ARIMA.

According to @hyndman2021forecasting, while exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data. The <tt>`ARIMA()`</tt> function combines unit root tests, minimisation of the AICc and MLE to obtain an ARIMA model. By setting <tt>`stepwise = FALSE`</tt> and <tt>`approximation = FALSE`</tt>, we are making R work extra hard to find a good model. 

```{r}
beer_arima <- beer_train %>%
  model(arima_auto = ARIMA(sales, stepwise = FALSE, approx = FALSE))
report(beer_arima)
```
The residuals for the best ARIMA model.

```{r}
#| label: fig-rftffam
#| fig-cap: "Residuals from the fitted fpp3 ARIMA(4,1,1)(0,1,1)[12] model."
beer_arima %>%
  select(arima_auto) %>%
  gg_tsresiduals()
```

The forecast for the best ARIMA model.

```{r}
beer_arima_fc <- beer_arima %>%
  fabletools::forecast(h = "10 months")
beer_arima_fc
```

The best ARIMA MAPE.

```{r}
arima_mape <- fabletools::accuracy(beer_arima_fc, beer_2022) %>%
  select(MAPE) %>%
  unlist()
arima_mape
```

Let's plot the forecast results.

```{r}
#| label: fig-faf
#| fig-cap: "fpp3: ARIMA(4,1,1)(0,1,1)[12] forecast."
beer_arima_fc %>%
  autoplot(beer_tbls, level = NULL) +
  geom_vline(xintercept = as.Date("2022-01-01"), lty = 2) +
  labs(y = "Sales", x = "Date") +
  scale_y_continuous(labels = scales::dollar)
```

The plot above is not very clear. Here is a zoom version.

```{r}
#| label: fig-zfaf
#| fig-cap: "Zoom. fpp3: ARIMA(4,1,1)(0,1,1)[12] forecast."
  ggplot(beer_zoom, aes(yearmonth(date), sales), lwd = 2, alpha = 0.4) +
  geom_line() +
  geom_point(size = 5, color = "black", alpha = 0.5, 
             shape = 21, fill = "black") +
  geom_point(aes(y = .mean), size = 5, 
             color = "red", alpha = 0.5, shape = 21, 
             fill = "red", data = beer_arima_fc) +
  geom_line(aes(y = .mean), color = "red", size = 0.5, data = beer_arima_fc) +
  geom_vline(xintercept = as.numeric(as.Date("2021-12-01")), 
               linetype = 2) +
  labs(x = "Date", y = "Sales",
       caption = c(paste("MAPE=",(round(arima_mape, 5))))) +
  theme(legend.position = "bottom") +
  scale_y_continuous(labels = scales::dollar)
```

Update the MAPE table.

```{r}
mape_updated <- mape_updated %>%
  add_row(.model = "fpp3: ARIMA(4,1,1)(0,1,1)[12]", MAPE = arima_mape) %>%
  arrange(desc(-MAPE))
mape_updated
```
## Neural network.

Artificial neural networks are forecasting methods that are based on simple mathematical models of the brain. They allow complex nonlinear relationships between the response variable and its predictors. With time series data, lagged values of the time series can be used as inputs to a neural network, just as we used lagged values in a linear autoregression model. We call this a neural network autoregression or NNAR model.

```{r}
beer_nnet <- beer_train %>%
  model(NNETAR(sales))
report(beer_nnet)
```
The NNAR(15,1,8)[12] model has inputs $y_{t−1}, y_{t−2},..., y_{t−15}$ and 8 neurons in the hidden layer. 

The forecast for the best NNAR model.

```{r}
beer_nnet_fc <- beer_nnet %>%
  fabletools::forecast(h = "10 months")
beer_nnet_fc
```

The best NNET MAPE.

```{r}
nnet_mape <- fabletools::accuracy(beer_nnet_fc, beer_2022) %>%
  select(MAPE) %>%
  unlist()
nnet_mape
```
Let's plot the forecast results.

```{r}
#| label: fig-fnf
#| fig-cap: "fpp3: NNAR(15,1,8)[12] forecast."
beer_nnet_fc %>%
  autoplot(beer_tbls, level = NULL) +
  geom_vline(xintercept = as.Date("2022-01-01"), lty = 2) +
  labs(y = "Sales", x = "Date") +
  scale_y_continuous(labels = scales::dollar)
```

The plot above is not very clear. Here is a zoom version.

```{r}
#| label: fig-zfnf
#| fig-cap: "Zoom. fpp3: NNAR(15,1,8)[12] forecast."
  ggplot(beer_zoom, aes(yearmonth(date), sales), lwd = 2, alpha = 0.4) +
  geom_line() +
  geom_point(size = 5, color = "black", alpha = 0.5, 
             shape = 21, fill = "black") +
  geom_point(aes(y = .mean), size = 5, 
             color = "red", alpha = 0.5, shape = 21, 
             fill = "red", data = beer_nnet_fc) +
  geom_line(aes(y = .mean), color = "red", size = 0.5, data = beer_nnet_fc) +
  geom_vline(xintercept = as.numeric(as.Date("2021-12-01")), 
               linetype = 2) +
  labs(x = "Date", y = "Sales",
       caption = c(paste("MAPE=",(round(nnet_mape, 5))))) +
  theme(legend.position = "bottom") +
  scale_y_continuous(labels = scales::dollar)
```

Update the MAPE table.

```{r}
mape_updated <- mape_updated %>%
  add_row(.model = "fpp3: NNAR(15,1,8)[12]", MAPE = nnet_mape) %>%
  arrange(desc(-MAPE))
mape_updated
```

```{r}
# I save the mape_updated object to use it in hh2o.rmd
saveRDS(mape_updated, "mape_updated.rds")
```

```{r}
# I save the forecast object to use it in hh2o.rmd
fpp3_fc <- bind_rows(beer_sn_fc, beer_ets_fc, beer_arima_fc,
                     beer_nnet_fc)
saveRDS(fpp3_fc, "fpp3_fc.rds")
```

